{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import edward as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in Edward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Models in Edward are inductively constructed as a static computation graph\n",
    "    * Each node can be a random variable, a deterministic computation step, or a neural network\n",
    "* Models can be composed by extending the graph at relevant nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primitive stochastic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use **Edward**'s models\n",
    "* Can create custom distributions using transforms or by inheriting directly from `RandomVariable`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from unit Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 0.   # mean zero\n",
    "scale = 1. # unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = ed.models.Normal(loc, scale) # creates a normal distribution object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0.162869\n",
      "log prob -0.9322017\n"
     ]
    }
   ],
   "source": [
    "x = normal.sample() # draws a sample from N(0, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    vx, vlp = sess.run([x, normal.log_prob(x)])\n",
    "    print(\"sample\", vx)\n",
    "    print(\"log prob\", vlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Samples are named hierarchically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample Tensor(\"Normal/x/Reshape:0\", shape=(), dtype=float32) 0.8472964\n"
     ]
    }
   ],
   "source": [
    "x = normal.sample(name=\"x\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    vx = sess.run(x)\n",
    "    print(\"sample\", x, vx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'cloudy', 38.88389)\n",
      "(b'sunny', 82.539665)\n",
      "(b'cloudy', 60.88809)\n"
     ]
    }
   ],
   "source": [
    "def weather():\n",
    "    cloudy = ed.models.Bernoulli(0.3).sample(name=\"cloudy\")\n",
    "    cloudy = tf.cond(tf.equal(cloudy, 1), lambda: tf.constant('cloudy'), lambda: tf.constant('sunny'))\n",
    "    mean_temp = tf.cond(tf.equal(cloudy, 'cloudy'), lambda: tf.constant(55.0), lambda: tf.constant(75.0))\n",
    "    scale_temp = tf.cond(tf.equal(cloudy, 'cloudy'), lambda: tf.constant(10.0), lambda: tf.constant(15.0))\n",
    "    temp = ed.models.Normal(mean_temp, scale_temp).sample(name='temp')\n",
    "    return cloudy, temp\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    (cloudy, temp) = weather()\n",
    "    for _ in range(3):\n",
    "        print(sess.run((cloudy, temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'cloudy', 63.0123, 45.523544)\n"
     ]
    }
   ],
   "source": [
    "def ice_cream_sales():\n",
    "    cloudy, temp = weather()\n",
    "    expected_sales = tf.cond(tf.equal(cloudy, \"sunny\"), lambda: tf.constant(200.0), lambda: tf.constant(50.0))\n",
    "    ice_cream = ed.models.Normal(expected_sales, 10.0).sample(name='ice_scream')\n",
    "    return cloudy, temp, ice_cream\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(ice_cream_sales()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Recursion, Higher-order Stochastic Functions, and Random Control Flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Edward** builds on **TensorFlow** and thus does not support functions nor general recursion.\n",
    "* Instead, it supports a restricted form of `while` loops (through `tf.while_loop`).\n",
    "* However, many recursive functions can still be converted into usages of `tf.while_loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "def geometric(p):\n",
    "    return tf.while_loop(\n",
    "        cond=lambda _: tf.equal(ed.models.Bernoulli(probs=p).sample(), 0), # name is automatically generated\n",
    "        body=lambda t: tf.add(t, 1),\n",
    "        loop_vars=[tf.constant(0)],\n",
    "    )\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acc = sess.run(geometric(0.001))\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [100%] ██████████████████████████████ Elapsed: 8s | Acceptance Rate: 0.988\n",
      "mean 9.120355\n",
      "stddev 0.6143609\n"
     ]
    }
   ],
   "source": [
    "def scale(guess):\n",
    "    var = 1.0\n",
    "    # The prior over weight encodes our uncertainty about our guess\n",
    "    weight = ed.models.Normal(guess, var)\n",
    "    # This encodes our belief about the noisiness of the scale:\n",
    "    # the measurement fluctuates around the true weight\n",
    "    measurement = ed.models.Normal(weight, 0.75)\n",
    "    return weight, measurement\n",
    "\n",
    "def experiment():\n",
    "    weight, measurement = scale(8.5)\n",
    "    posterior = ed.models.Empirical(tf.Variable(tf.zeros(10000)))\n",
    "    inference = ed.HMC({weight: posterior}, {measurement: 9.5})\n",
    "    tf.global_variables_initializer().run()\n",
    "    inference.run()\n",
    "    vmean, vstd = ed.get_session().run((posterior.mean(), posterior.stddev()))\n",
    "    print(\"mean\", vmean)\n",
    "    print(\"stddev\", vstd)\n",
    "\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 1.182\n",
      "guess 9.500665\n",
      "mean 9.500643\n",
      "stddev 0.5976114\n"
     ]
    }
   ],
   "source": [
    "def guide(guess):\n",
    "    qalpha = tf.Variable(ed.models.Normal(0.0, 1.0).sample() + guess, name='a')\n",
    "    qbeta = tf.Variable(ed.models.Normal(0.0, 1.0).sample(), name='b')\n",
    "    qweight = ed.models.Normal(qalpha, qbeta)\n",
    "    return qalpha, qbeta, qweight\n",
    "\n",
    "def vi_experiment():\n",
    "    guess = tf.Variable(8.5)\n",
    "    weight, measurement = scale(guess)\n",
    "    qalpha, qbeta, qweight = guide(guess)\n",
    "    inference = ed.KLqp({weight: qweight}, {measurement: 9.5})\n",
    "    tf.global_variables_initializer().run()\n",
    "    inference.run(n_samples=10, n_iter=10000)\n",
    "    vguess, vmean, vstd = ed.get_session().run((guess, qalpha, qbeta))\n",
    "    print('guess', vguess)\n",
    "    print(\"mean\", vmean)\n",
    "    print(\"stddev\", vstd)\n",
    "\n",
    "vi_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 206.073\n",
      "Epoch: 2\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 154.291\n",
      "Epoch: 3\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 144.893\n",
      "Epoch: 4\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 139.864\n",
      "Epoch: 5\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 11s\n",
      "-log p(x) <= 135.654\n",
      "Epoch: 6\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 131.181\n",
      "Epoch: 7\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 126.816\n",
      "Epoch: 8\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 122.847\n",
      "Epoch: 9\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 119.230\n",
      "Epoch: 10\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 116.200\n",
      "Epoch: 11\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 113.931\n",
      "Epoch: 12\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 112.039\n",
      "Epoch: 13\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 110.224\n",
      "Epoch: 14\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 108.915\n",
      "Epoch: 15\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 107.773\n",
      "Epoch: 16\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 106.855\n",
      "Epoch: 17\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 106.224\n",
      "Epoch: 18\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 105.741\n",
      "Epoch: 19\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 105.234\n",
      "Epoch: 20\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 104.824\n",
      "Epoch: 21\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 104.615\n",
      "Epoch: 22\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 104.206\n",
      "Epoch: 23\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 103.981\n",
      "Epoch: 24\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 103.619\n",
      "Epoch: 25\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 103.450\n",
      "Epoch: 26\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 103.173\n",
      "Epoch: 27\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 103.002\n",
      "Epoch: 28\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 102.844\n",
      "Epoch: 29\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 102.695\n",
      "Epoch: 30\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 102.570\n",
      "Epoch: 31\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 102.419\n",
      "Epoch: 32\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 102.322\n",
      "Epoch: 33\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 102.124\n",
      "Epoch: 34\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 102.080\n",
      "Epoch: 35\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.942\n",
      "Epoch: 36\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.795\n",
      "Epoch: 37\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.745\n",
      "Epoch: 38\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.601\n",
      "Epoch: 39\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.479\n",
      "Epoch: 40\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.394\n",
      "Epoch: 41\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.302\n",
      "Epoch: 42\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.233\n",
      "Epoch: 43\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.182\n",
      "Epoch: 44\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.138\n",
      "Epoch: 45\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.019\n",
      "Epoch: 46\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 101.046\n",
      "Epoch: 47\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.973\n",
      "Epoch: 48\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.982\n",
      "Epoch: 49\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.764\n",
      "Epoch: 50\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.749\n",
      "Epoch: 51\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.700\n",
      "Epoch: 52\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.643\n",
      "Epoch: 53\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.613\n",
      "Epoch: 54\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 11s\n",
      "-log p(x) <= 100.549\n",
      "Epoch: 55\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.461\n",
      "Epoch: 56\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.417\n",
      "Epoch: 57\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.349\n",
      "Epoch: 58\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.308\n",
      "Epoch: 59\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.252\n",
      "Epoch: 60\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.293\n",
      "Epoch: 61\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.242\n",
      "Epoch: 62\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.175\n",
      "Epoch: 63\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.088\n",
      "Epoch: 64\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 100.098\n",
      "Epoch: 65\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 9s\n",
      "-log p(x) <= 100.122\n",
      "Epoch: 66\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.996\n",
      "Epoch: 67\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.947\n",
      "Epoch: 68\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.980\n",
      "Epoch: 69\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.907\n",
      "Epoch: 70\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.862\n",
      "Epoch: 71\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.850\n",
      "Epoch: 72\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.770\n",
      "Epoch: 73\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.750\n",
      "Epoch: 74\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.692\n",
      "Epoch: 75\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.816\n",
      "Epoch: 76\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.670\n",
      "Epoch: 77\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.648\n",
      "Epoch: 78\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.647\n",
      "Epoch: 79\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.617\n",
      "Epoch: 80\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.554\n",
      "Epoch: 81\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.586\n",
      "Epoch: 82\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.494\n",
      "Epoch: 83\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.505\n",
      "Epoch: 84\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.443\n",
      "Epoch: 85\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.453\n",
      "Epoch: 86\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.412\n",
      "Epoch: 87\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.380\n",
      "Epoch: 88\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.383\n",
      "Epoch: 89\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.374\n",
      "Epoch: 90\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.300\n",
      "Epoch: 91\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.270\n",
      "Epoch: 92\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.267\n",
      "Epoch: 93\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.258\n",
      "Epoch: 94\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.278\n",
      "Epoch: 95\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.267\n",
      "Epoch: 96\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.207\n",
      "Epoch: 97\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.163\n",
      "Epoch: 98\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.143\n",
      "Epoch: 99\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.142\n",
      "Epoch: 100\n",
      "234/234 [100%] ██████████████████████████████ Elapsed: 10s\n",
      "-log p(x) <= 99.112\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "def generator(array, batch_size):\n",
    "    \"\"\"Generate batch with respect to array's first axis.\"\"\"\n",
    "    start = 0  # pointer to where we are in iteration\n",
    "    while True:\n",
    "        stop = start + batch_size\n",
    "        diff = stop - array.shape[0]\n",
    "        if diff <= 0:\n",
    "            batch = array[start:stop]\n",
    "            start += batch_size\n",
    "        else:\n",
    "            batch = np.concatenate((array[start:], array[:diff]))\n",
    "            start = diff\n",
    "        batch = batch.astype(np.float32) / 255.0  # normalize pixel intensities\n",
    "        batch = np.random.binomial(1, batch)  # binarize images\n",
    "        yield batch.reshape(-1, 784) # shape the mini-batch to have pixels in the rightmost dimension\n",
    "\n",
    "\n",
    "def vae(nepoch, batch_size, latent_dimension, learning_rate):\n",
    "    (x_train, _),(x_test, _) = mnist.load_data()\n",
    "    x_train_generator = generator(x_train, batch_size)\n",
    "    # decoder\n",
    "    z = ed.models.Normal(tf.zeros([batch_size, latent_dimension]), tf.ones([batch_size, latent_dimension]))\n",
    "    hidden = tf.layers.dense(z, 256, activation=tf.nn.relu)\n",
    "    x = ed.models.Bernoulli(logits=tf.layers.dense(hidden, 28 * 28))\n",
    "    # encoder\n",
    "    x_ph = tf.placeholder(tf.int32, [batch_size, 28 * 28])\n",
    "    hidden = tf.layers.dense(tf.cast(x_ph, tf.float32), 256, activation=tf.nn.relu)\n",
    "    qz = ed.models.Normal(tf.layers.dense(hidden, latent_dimension),\n",
    "                tf.layers.dense(hidden, latent_dimension, activation=tf.nn.softplus))\n",
    "    # bind decoder and encoder\n",
    "    inference = ed.KLqp({z: qz}, data={x: x_ph})\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    inference.initialize(optimizer=optimizer)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    n_iter_per_epoch = x_train.shape[0] // batch_size\n",
    "    for epoch in range(1, nepoch + 1):\n",
    "        print(\"Epoch: {0}\".format(epoch))\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        pbar = ed.util.Progbar(n_iter_per_epoch)\n",
    "        for t in range(1, n_iter_per_epoch + 1):\n",
    "            pbar.update(t)\n",
    "            x_batch = next(x_train_generator)\n",
    "            info_dict = inference.update(feed_dict={x_ph: x_batch})\n",
    "            avg_loss += info_dict['loss']\n",
    "\n",
    "        # print a lower bound to the average marginal likelihood for an image\n",
    "        avg_loss /= n_iter_per_epoch\n",
    "        avg_loss /= batch_size\n",
    "        print(\"-log p(x) <= {:0.3f}\".format(avg_loss))\n",
    "\n",
    "vae(100, 256, 400, 1.0e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
