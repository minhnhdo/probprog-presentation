{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyprob\n",
    "from pyprob import Model\n",
    "from pyprob.distributions import Normal, Uniform\n",
    "\n",
    "pyprob.set_verbosity(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyProb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **PyProb** is a PyTorch-based probabilistic programming language\n",
    "* It supports **inference compilation** with minimal intervention\n",
    "* It uses a protocol (PPX) that allows **distributed** training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primitive Stochastic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Built on top of **PyTorch's** distributions library\n",
    "* API is, however, slightly different than Pyro's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing sample from unit Normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.\n",
    "sigma = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = Normal(mu, sigma) # create a normal distribution object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample tensor(-0.3696)\n",
      "log prob tensor(-0.9873)\n"
     ]
    }
   ],
   "source": [
    "x = normal.sample() # draw a sample from N(0,1)\n",
    "print(\"sample\", x)\n",
    "print(\"log prob\", normal.log_prob(x)) # score the sample from N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Just like in the Pyro examples, these distributions and sampling functions are backed by the underlying PyTorch implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2404)\n"
     ]
    }
   ],
   "source": [
    "x = Normal(mu, sigma).sample()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No need to explicitly pass a name to the random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ice-Cream Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s suppose we have a bunch of data with daily mean temperatures and cloud cover. We want to reason about how temperature interacts with whether it was sunny or cloudy. A simple stochastic function that does that is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sunny', tensor(83.3104))\n",
      "('cloudy', tensor(62.5814))\n",
      "('cloudy', tensor(47.2370))\n"
     ]
    }
   ],
   "source": [
    "def weather():\n",
    "    cloudy = Uniform(0, 1).sample()\n",
    "    cloudy = 'cloudy' if cloudy < 0.3 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = Normal(mean_temp, scale_temp).sample()\n",
    "    return cloudy, temp\n",
    "    \n",
    "for _ in range(3):\n",
    "    print(weather())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ice_cream_sales():\n",
    "    cloudy, temp = weather()\n",
    "    expected_sales = 200. if cloudy == 'sunny' and temp > 80.0 else 50.\n",
    "    return Normal(expected_sales, 10.0).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(186.1586)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_cream_sales()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference: One Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianUnknownMean(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"Gaussian with unknown mean\") # give the model a name\n",
    "        \n",
    "        # prior mean and std\n",
    "        self.prior_mean = 1\n",
    "        self.prior_stdd = math.sqrt(5)\n",
    "        self.likelhood_stdd = math.sqrt(2)\n",
    "\n",
    "    # Needed to specifcy how the generative model is run forward\n",
    "    def forward(self):\n",
    "        # sample the (latent) mean variable to be inferred:\n",
    "        mu = pyprob.sample(Normal(self.prior_mean, self.prior_stdd))\n",
    "\n",
    "        # define the likelihood\n",
    "        likelihood = Normal(mu, self.likelhood_stdd)\n",
    "\n",
    "\n",
    "        # observation \"placeholders\"\n",
    "        # these can have concrete values associated with them later when\n",
    "        # conditioning on data.\n",
    "        pyprob.observe(likelihood, name='observation-1')\n",
    "        pyprob.observe(likelihood, name='observation-2')\n",
    "\n",
    "        # return the latent quantity of interest\n",
    "        return mu\n",
    "\n",
    "# create an instance of the model\n",
    "gum = GaussianUnknownMean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The probabilistic program above models **Gaussian with Unknown Mean**: our goal is to infer the expected value of `mu` above.\n",
    "* PyProb supports multiple inference algorithms, including inference compilation (discussed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "mean: tensor(7.1997)\n",
      "stddev: tensor(0.6403)\n"
     ]
    }
   ],
   "source": [
    "posterior = gum.posterior_distribution(\n",
    "    num_traces=1000, # number of samples to be used\n",
    "    inference_engine=pyprob.InferenceEngine.IMPORTANCE_SAMPLING, # the inference algorithm\n",
    "    observe={'observation-1': 9, 'observation-2': 8.4} # conditioning on some observations\n",
    ")\n",
    "\n",
    "# inferring the mean and standard deviation of our `mu` latent variable\n",
    "print(\"mean:\", posterior.mean)\n",
    "print(\"stddev:\", posterior.stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PyProb provides other inference algorithms too:\n",
    "    * Importance sampling\n",
    "    * Lightweight Metropolis-Hastings\n",
    "    * Random Walk Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyProb supports **inference compilation**. Let's apply that to the _Gaussian with Unknown Mean_ example we used in the last section. Recall that we defined a model for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GaussianUnknownMean at 0x7f4cb16bf4a8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyProb provides an inference engine to use in combination with a **learned inference network**.\n",
    "\n",
    "First, we need to learn the inference network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new inference network...\n",
      "Initializing inference network...\n",
      "Observable observation-1: observe embedding not specified, using the default FEEDFORWARD.\n",
      "Observable observation-1: embedding depth not specified, using the default 2.\n",
      "Observable observation-2: observe embedding not specified, using the default FEEDFORWARD.\n",
      "Observable observation-2: embedding depth not specified, using the default 2.\n",
      "New proposal layer for address: 16__forward__mu__Normal__1\n",
      "Total number of parameters: 5,834\n",
      "Train. time | Trace     | Init. loss| Min. loss | Curr. loss| T.since min | Traces/sec\n",
      "0d:00:00:02 | 1,024     | +2.26e+00 | +2.10e+00 | \u001b[31m+2.26e+00\u001b[0m | 0d:00:00:01 | 593.0                              \n"
     ]
    }
   ],
   "source": [
    "gum.learn_inference_network(\n",
    "    num_traces=1000, # how many traces (executions of the generative model) to use to train the inference network\n",
    "    observe_embeddings={\n",
    "        'observation-1': {'dim': 10},\n",
    "        'observation-2': {'dim': 10}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the inference network is **trained** with the traces produced by the generative model, we can use it to perform **inference**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor(7.2872)\n",
      "stddev: tensor(0.7825)\n"
     ]
    }
   ],
   "source": [
    "posterior = gum.posterior_distribution(\n",
    "    num_traces=1000,\n",
    "    inference_engine=pyprob.InferenceEngine.IMPORTANCE_SAMPLING_WITH_INFERENCE_NETWORK,\n",
    "    observe={'observation-1': 9, 'observation-2': 8.4}\n",
    ")\n",
    "\n",
    "# inferring the mean and standard deviation of our `mu` latent variable\n",
    "print(\"mean:\", posterior.mean)\n",
    "print(\"stddev:\", posterior.stddev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
